# -*- coding: utf-8 -*-
"""Prodigy_Infotech_DS_Task-04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u-l1iFzpcIoBcno12pGRiPpxqtUAdhFc

# Prodigy Infotech

Author: Aarti Wani

Data Science Intern

Task-04

Task: Analyze and visualize sentiment patterns in social media data to understand public opinion and attitudes towards specific topics or brands.
"""

# Importing libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import string
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score,classification_report

# Loading training and testin dataset
col=['Tweet_ID','Entity','Sentiment','Tweet_content']
train_data=pd.read_csv('twitter_training.csv',names=col)
test_data=pd.read_csv('twitter_validation.csv',names=col)
train_data.head() # Checking first five records of training dataset

# Checking first five records of testing dataset
test_data.head()

# Checking the dimensions of training dataset
train_data.shape

# Checking the dimensions of testing dataset
test_data.shape

# Checking the datatypes of features
train_data.info()

# Checking missing values of training dataset
train_data.isna().sum()

# Checking the duplicate values
train_data.duplicated().sum()

# removing missing values
train_data=train_data.dropna()
train_data.isna().sum()

# removing duplicate values
train_data=train_data.drop_duplicates()
train_data.duplicated().sum()

# Checking missing values in testing dataset
test_data.isna().sum()

# Checking duplicate values in testing dataset
test_data.duplicated().sum()

# Checking the distribution of sentiment
train_data['Sentiment'].value_counts()

# Checking percentages of sentients in tweet
palette=sns.color_palette('Paired')
plt.pie(x=train_data['Sentiment'].value_counts(),labels=['Negative','Positive','Neutral','Irrelevant'],autopct='%.0f%%',colors=palette)
plt.title('Distribution of Sentiment')
# It is clear that negative sentiments are more in a tweeet.

# Checking the distribution of entity
train_data['Entity'].value_counts()

df=train_data.groupby(['Entity','Sentiment']).count()

plt.figure(figsize=(25,10))
sns.barplot(train_data, x = "Entity", y = "Tweet_ID",hue = "Sentiment",palette='Paired')
plt.xticks(rotation=90)
plt.ylabel("No. of tweets")
plt.title("Tweet per Entity")
# from graph it is clear that 'Xbox' entity had more tweets

# Visualizing first 15 twitter entity
top15_entity=train_data['Entity'].value_counts().sort_values(ascending=False)[:15]
plt.figure(figsize=(25,10))
ax=sns.barplot(x = top15_entity.index, y = top15_entity.values)
ax.bar_label(ax.containers[0], label_type='edge', padding=5)
plt.xlabel("Entity")
plt.ylabel("No. of tweets")
plt.title("Top 15 Twitter Entity Distribution")
plt.show()
# it is clear that out of first 15 entities TomClancyRainbbowSix had more tweets

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Visualizing the important words for sentiment
import re
from nltk.corpus import stopwords
stopwords_list = stopwords.words('english')

word_counts = {'Positive': [],
                'Neutral': [],
                'Irrelevant': [],
                'Negative': []}

pattern = re.compile('[^\w ]')
for text, t in zip(train_data['Tweet_content'], train_data['Sentiment']):
    text = re.sub(pattern, '', text).lower().split()
    text = [word for word in text if word not in stopwords_list]
    word_counts[t].extend(text)

fig, axes = plt.subplots(2, figsize=(10,16))
for axis, (target, words) in zip(axes.flatten(), word_counts.items()):
    bar_info = pd.Series(words).value_counts()[:25]
    sns.barplot(x=bar_info.values, y=bar_info.index, ax=axis)
    axis.set_title(f'Words for {target}')
    axis.set_ylabel('words')
plt.show()

# Heatmap
data1=pd.crosstab(train_data['Entity'], train_data['Sentiment'])

plt.figure(figsize=(10, 10))
sns.heatmap(data1, annot=True,cmap='Paired',fmt="")

# Data cleaning
def preprocess_text(text):
    # Handling NaN values
    if isinstance(text, float) and np.isnan(text):
        return ""

    # Converting text to lowercase
    text = text.lower()

    # Removing special characters, URLs, and mentions
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@[A-Za-z0-9]+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    return text

#Encoding
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
train_data['Sentiment'] = label_encoder.fit_transform(train_data['Sentiment'])
test_data['Sentiment'] = label_encoder.fit_transform(test_data['Sentiment'])
train_data.head()

train_data['Tweet_content'] = train_data['Tweet_content'].apply(preprocess_text)
test_data['text'] = test_data['Tweet_content'].apply(preprocess_text)

# Feature Extraction
tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')
X_train = tfidf_vectorizer.fit_transform(train_data['Tweet_content'])
X_test = tfidf_vectorizer.transform(test_data['Tweet_content'])

# vectorizer=TfidfVectorizer()
# X = vectorizer.fit_transform(train_data['Tweet_content']).toarray()
y_train = train_data['Sentiment']
y_test=test_data['Sentiment']

# Applying model and making predictions
dt=DecisionTreeClassifier()
dt.fit(X_train,y_train)
y_pred=dt.predict(X_test)
y_pred

# Checking accuracy
print(accuracy_score(y_test,y_pred))
print(classification_report(y_test,y_pred))